{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression test on MyModel -> GPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import signatory\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "def phi(x, C, a):\n",
    "  '''\n",
    "  x: signature norm squared\n",
    "  C: phi function parameter, it controls how strict the normalization is\n",
    "  a: phi function parameter, tipically fixed to 1 in order to avoid too much hyperparameter\n",
    "  '''\n",
    "  if x > C:\n",
    "    return C + (C**(1+a))*(C**(-a) - x**(-a))/a\n",
    "  else:\n",
    "    return x\n",
    "  \n",
    "\n",
    "def dilatation(x, C, a, M, d):\n",
    "  '''\n",
    "  x: an array with dimension batch x signature\n",
    "  C, a : phi function parameters\n",
    "  M : truncation level (called L in the paper)\n",
    "  d : dimension of the time series we are computing the signature of (!! if time augmentation is deployed, then the dimension is increased by 1)\n",
    "  '''\n",
    "  xNumpy = x.detach().numpy()\n",
    "\n",
    "  coefficients = np.zeros((xNumpy.shape[0], (M+1)))\n",
    "  normalizz = np.zeros((xNumpy.shape[0], 1))\n",
    "\n",
    "  for i in range(0,xNumpy.shape[0]):\n",
    "      normQuad = 1 + np.sum(xNumpy[i]**2) #signature norm squared\n",
    "      coefficients[i, 0] = 1-phi(normQuad,C,a)\n",
    "      for j in range(1, (M+1)):\n",
    "         coefficients[i, j] = np.sum(xNumpy[i, int(((d**j-1)/(d-1)-1)):int(((d**(j+1)-1)/(d-1)-1))]**2)\n",
    "      def polin(input):\n",
    "        xMonomials = np.zeros((M+1)) + 1\n",
    "        for k in range(1, (M+1)):\n",
    "            xMonomials[k] = input**(2*k)\n",
    "        return np.dot(xMonomials, coefficients[i])\n",
    "      normalizz[i] = optimize.brentq(polin, 0, 2)\n",
    "      if normalizz[i] > 1:\n",
    "        normalizz[i] = 1\n",
    "  return torch.from_numpy(normalizz)\n",
    "\n",
    "\n",
    "''' grad computation of dilatation function (corollary B.14 in the paper) '''\n",
    "class Normalization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, input, C, M, d, exponents): #a=1, input should have dimension batch x length_sign, M is the truncation level, d the dimension of the timeseries, C normalization constant, exponents is useful to define correctly the dilatation\n",
    "      '''\n",
    "      input: an array batch x signature, it is x in the previous function\n",
    "      C, M, d: as in the previous function\n",
    "      exponents: a vector as long as the signature, it has d times 1, d^2 times 2,..., d^M times the value M\n",
    "      '''\n",
    "      norm = dilatation(input, C, 1, M, d)\n",
    "      self.save_for_backward(input, exponents)\n",
    "      self.C = C\n",
    "      self.M = M\n",
    "      self.d = d\n",
    "      self.norm = norm # batch x 1\n",
    "      return norm.to(torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "      '''\n",
    "      grad_output: grad of upper layers\n",
    "      '''\n",
    "      input,exponents = self.saved_tensors\n",
    "      normToSquarePower = (self.norm**(2*exponents)).to(torch.float64) #batch x length_sign\n",
    "      normToSquarePowerMinus1 = (self.norm**(2*exponents-1)).to(torch.float64) #batch x length_sign\n",
    "      denominator = torch.sum((input**2)*(normToSquarePowerMinus1),1,keepdim=True) #batch x 1\n",
    "      inputnorm = (1+torch.sum(input**2, 1, keepdim=True))**(1/2) # batch x 1 ,norm of the pre-normalized signature\n",
    "\n",
    "      # computing phi derivative based on the 2 branches of the phi function\n",
    "      phiDerivative = torch.zeros(input.shape[0], 1, dtype=torch.float64) #batch x 1\n",
    "      \n",
    "      # second branch\n",
    "      index1 = torch.where(inputnorm[:,0]>(self.C)**(1/2))[0]\n",
    "      if len(index1) > 0:\n",
    "        phiDerivative[index1,:] = 2*(self.C)**2/(inputnorm[index1,:]**3)\n",
    "      \n",
    "      # first branch\n",
    "      index2=torch.where(inputnorm[:,0]<=(self.C)**(1/2))[0]\n",
    "      if len(index2) > 0:\n",
    "        phiDerivative[index2,:] = 2*inputnorm[index2,:]\n",
    "      \n",
    "      # Numerator\n",
    "      Numerator = normToSquarePower - (phiDerivative/(2*inputnorm))*torch.ones(input.shape[0], int((self.d**(self.M+1)-1)/(self.d-1)-1)) # batch x length_sign\n",
    "      Numerator = input*Numerator\n",
    "\n",
    "      gradient = Numerator/denominator # batch x length_sign\n",
    "      return grad_output*gradient, None, None, None, None\n",
    "  \n",
    "''' Triangular: transformed a vector into a lower triangular matrix '''\n",
    "class Triangular(torch.nn.Module):\n",
    "  def __init__(self, dim, L2, x_indices, y_indices):\n",
    "    '''\n",
    "    dim: dimension of the starting time series\n",
    "    L2: number of new time instants\n",
    "    x_indices, y_indices: explained in the model construction (below)\n",
    "    '''\n",
    "    super(Triangular, self).__init__()\n",
    "    self.dim = dim\n",
    "    self.L2 = L2\n",
    "    self.x_indices = x_indices\n",
    "    self.y_indices = y_indices\n",
    "\n",
    "\n",
    "  def forward(self,x): # x is of size batch x int((int(L2*(L2+1)/2)-int((L2-alp)*(L2-alp+1)/2))*(int(dim*(dim+1)/2)))\n",
    "    A = torch.zeros((x.shape[0], self.L2*self.dim, self.L2*self.dim))\n",
    "    A[:,torch.Tensor.long(self.x_indices), torch.Tensor.long(self.y_indices)] = x\n",
    "    return A\n",
    "\n",
    "\n",
    "''' Preparation with time augmentation: combines original time series and values sampled, then applies time augmentation '''\n",
    "class PreparationWithTimeAugmentation(torch.nn.Module):\n",
    "  def __init__(self, order, timesteps_cut, dim, extended_order):\n",
    "    '''\n",
    "    dim: as in the previous function\n",
    "    timesteps_cut: number of time steps, sum of known and new time steps\n",
    "    order, extended_order: explained in the model\n",
    "    '''\n",
    "    super(PreparationWithTimeAugmentation, self).__init__()\n",
    "    self.order = order\n",
    "    self.extended_order = extended_order\n",
    "    self.cut = timesteps_cut\n",
    "    self.d = dim\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    '''    \n",
    "    x: starting time series\n",
    "    y: new values sampled\n",
    "    '''\n",
    "    timesteps = x[:, :self.cut] # time instants: before known and then new ones\n",
    "    values = x[:, self.cut:] # starting time series\n",
    "    values = torch.cat((values, y), 1) # concatenate values with the new values\n",
    "    \n",
    "    # reorder values\n",
    "    values = values[:, self.extended_order.type(torch.LongTensor)]\n",
    "    values = values.reshape([values.shape[0], self.cut, self.d])\n",
    "    \n",
    "    # adding time component\n",
    "    timesteps = timesteps[:, self.order]\n",
    "    path = torch.cat((values, timesteps.unsqueeze(2)), 2)\n",
    "    return path\n",
    "\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "  def __init__(self, L1, L2, dim, order, extended_order, alpha, level, number_classes, C, a, K):\n",
    "    '''\n",
    "    L1: number of known time instants, i.e. length of the time series\n",
    "    L2: number of new time instants\n",
    "    order: in Preparationwithtimeaugmentation we concatenate the starting values and the sampled one, order reorganize them (FOR EXAMPLE L1=100, L2=99, THEN ORDER=[0,100,1,101,2,102,..] IF THE NEW TIME INSTANTS ARE THE MIDDLE POINTS)\n",
    "    extended_order: as order but takes into account the dimension of the time series (FOR EXAMPLE D=2, L1=100,L2=99 SO STARTING TIME SERIES HAS 200 VALUES AND NEW VALUES ARE 198, SO EXTENDED ORDER=[0,1,200,201,2,3,202,203,...])\n",
    "    ALP: how many subdiagonals of the lower triangular matrix are non zero, alpha=L2 means no zero in the lower part\n",
    "    level: signature truncation level\n",
    "    number_classes: number of labels in the classification problem\n",
    "    C, a: phi function parameters\n",
    "    K: numbers of augmented paths generated\n",
    "    dim: dimension of the starting time series\n",
    "    '''\n",
    "    super(MyModel, self).__init__()\n",
    "    self.K = K\n",
    "    self.C = C\n",
    "    self.a = a\n",
    "    self.alpha = alpha\n",
    "    self.L1 = L1\n",
    "    self.L2 = L2\n",
    "    self.dim = dim\n",
    "    self.order = order\n",
    "    self.extended_order = extended_order\n",
    "\n",
    "    # compute how much elements in the lower triangular matrix\n",
    "    MatrixEl = int((int(L2*(L2+1)/2) - int((L2-alpha)*(L2-alpha+1)/2)) * (int(dim*(dim+1)/2)))\n",
    "    self.level = level\n",
    "    self.number_classes = number_classes\n",
    "\n",
    "    # number of components of the signature (remember we have time augmentation)\n",
    "    self.outputSigDim = int(((dim+1)**(level+1)-1)/(dim)-1)\n",
    "    # bulding the exponents useful for normalization procedure\n",
    "    self.exponents = torch.ones(int(((dim+1)**(level+1)-1)/dim-1)).to(torch.float64)\n",
    "    for j in range(2, (level+1)):\n",
    "        self.exponents[int((((dim+1)**j-1)/(dim)-1)):int((((dim+1)**(j+1)-1)/(dim)-1))] = torch.ones((int(dim+1)**j))*j\n",
    "\n",
    "    #needed to reshape first layer output into a matrix (in triangular function)\n",
    "    tril_indices = torch.tril_indices(row=(dim), col=(dim), offset=0)\n",
    "    x_indices = torch.zeros(int(L2*(L2+1)/2) - int((L2-alpha)*(L2-alpha+1)/2), dtype=torch.int32)\n",
    "    for i in range(alpha):\n",
    "      x_indices[i*L2-int(i*(i-1)/2):(i+1)*L2-int((i+1)*i/2)] = torch.arange(i, L2, 1)\n",
    "\n",
    "    y_indices=torch.zeros(int(L2*(L2+1)/2) - int((L2-alpha)*(L2-alpha+1)/2), dtype=torch.int32)\n",
    "    for i in range(alpha):\n",
    "      y_indices[i*L2-int(i*(i-1)/2):(i+1)*L2-int((i+1)*i/2)] = torch.arange(0, L2-i, 1)\n",
    "\n",
    "    self.x_indicesFull = torch.zeros(int((int(L2*(L2+1)/2) - int((L2-alpha)*(L2-alpha+1)/2))*(int(dim*(dim+1)/2))), dtype=torch.int32)\n",
    "    self.y_indicesFull = torch.zeros(int((int(L2*(L2+1)/2) - int((L2-alpha)*(L2-alpha+1)/2))*(int(dim*(dim+1)/2))), dtype=torch.int32)\n",
    "    for j in range(x_indices.shape[0]):\n",
    "      self.x_indicesFull[(j*int(dim*(dim+1)/2)):((j+1)*int(dim*(dim+1)/2))] = (x_indices[j]*dim) + tril_indices[0]\n",
    "      self.y_indicesFull[(j*int(dim*(dim+1)/2)):((j+1)*int(dim*(dim+1)/2))] = (y_indices[j]*dim) + tril_indices[1]\n",
    "\n",
    "    self.MeanLayer = torch.nn.Linear((L1*(dim+1)+L2), (L2*dim)) # (L1 + L2 + L1*d, L2*d)\n",
    "    self.SqrtCovLayer = torch.nn.Linear((L1*(dim+1)+L2), MatrixEl)\n",
    "    self.FinalLayer = torch.nn.Linear(self.outputSigDim, self.number_classes)\n",
    "    self.GaussianSampler = torch.distributions.Normal(0, 1)\n",
    "    self.SignatureLayer = signatory.Signature(self.level)\n",
    "    self.LogSoftmax = torch.nn.LogSoftmax(1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = self.MeanLayer(x)\n",
    "    sqrtCov = self.SqrtCovLayer(x)\n",
    "    sqrtCovMatrix = Triangular(self.dim, self.L2, self.x_indicesFull, self.y_indicesFull)(sqrtCov)\n",
    "    epsilon = self.GaussianSampler.sample(torch.Size([x.shape[0], self.L2 * self.dim, self.K]))\n",
    "    newValues = torch.bmm(sqrtCovMatrix, epsilon) + mean.unsqueeze(2)\n",
    "\n",
    "    signatures = torch.zeros(x.shape[0], self.outputSigDim, self.K)\n",
    "    for i in range(self.K):\n",
    "      path = PreparationWithTimeAugmentation(self.order, self.L1 + self.L2, self.dim, self.extended_order)(x, newValues[:, :, i])\n",
    "      sig = self.SignatureLayer(path)\n",
    "      norm = Normalization.apply(sig.to(torch.float64), self.C, self.level, self.dim + 1, self.exponents)\n",
    "      signatures[:, :, i] = (sig * (norm**self.exponents)).type(torch.float32)\n",
    "\n",
    "    self.MeanSig = torch.mean(signatures, 2)\n",
    "    output = self.FinalLayer(self.MeanSig)\n",
    "    output = self.LogSoftmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model import GPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin, end, number, division, dim = 0, 1, 100, 1, 1\n",
    "'''\n",
    "begin = first time steps\n",
    "end = last known time steps\n",
    "division = 1 means we are taking the middle points as new time instants -> L2 = L1 - 1\n",
    "number = L1\n",
    "dim = 1 - one dimensional\n",
    "'''\n",
    "\n",
    "# generating the time steps\n",
    "known_times = torch.linspace(begin, end, number)\n",
    "new_times = torch.zeros(division*(number-1))\n",
    "for i in range(0,(number-1)):\n",
    "  new_times[(division*i):(division*(i+1))] = torch.linspace(known_times[i], known_times[i+1], (division+2))[1:(1 + division)]\n",
    "\n",
    "# Length of known values and new values\n",
    "L1 = known_times.shape[0]\n",
    "L2 = new_times.shape[0]\n",
    "\n",
    "timesteps = torch.cat((known_times, new_times),axis=0)\n",
    "timesteps_sorted, order = torch.sort(timesteps)\n",
    "\n",
    "extended_order = torch.zeros(dim*order.size(0))\n",
    "for i in range(order.size(0)):\n",
    "  extended_order[(i*dim):((i+1)*dim)] = torch.arange(order[i]*dim, (order[i] + 1)*dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "x = torch.rand(batch_size, L1 + L2 + L1)\n",
    "y = (torch.rand(batch_size) > 0.5).long()\n",
    "\n",
    "alpha = 5\n",
    "level = 3\n",
    "number_classes = 2\n",
    "C = 5e3\n",
    "a = 1\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOld = MyModel(\n",
    "    L1=L1, \n",
    "    L2=L2, \n",
    "    dim=dim, \n",
    "    order=order, \n",
    "    extended_order=extended_order, \n",
    "    alpha=alpha, \n",
    "    level=level, \n",
    "    number_classes=number_classes, \n",
    "    C=C, \n",
    "    a=a, \n",
    "    K=K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPES(\n",
    "    L1=L1, \n",
    "    L2=L2, \n",
    "    dim=dim, \n",
    "    order=order, \n",
    "    extended_order=extended_order, \n",
    "    alpha=alpha, \n",
    "    level=level, \n",
    "    number_classes=number_classes, \n",
    "    C=C, \n",
    "    a=a, \n",
    "    K=K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same initialization\n",
    "\n",
    "model.load_state_dict(modelOld.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass reg-test\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "yhatOld = modelOld(x)\n",
    "torch.random.manual_seed(0)\n",
    "yhat = model(x)\n",
    "\n",
    "assert torch.allclose(yhatOld, yhat)\n",
    "\n",
    "# backward pass reg-test\n",
    "\n",
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lossOld = criterion(yhatOld, y)  # Compute loss\n",
    "lossOld.backward()  # Backward pass\n",
    "\n",
    "loss = criterion(yhat, y)  # Compute loss\n",
    "loss.backward()  # Backward pass\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pOld, p in zip(modelOld.parameters(), model.parameters()):\n",
    "        assert torch.allclose(pOld, p)\n",
    "        assert torch.allclose(pOld.grad, p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting GPU experiments on RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/rds/general/user/ll1917/home/esig/gp-esig-classifier') # to add when running on remote Jupyter server\n",
    "os.chdir('/rds/general/user/ll1917/home/esig/gp-esig-classifier') # to add when running on remote Jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: which: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `which'\n",
      "/bin/bash: module: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `module'\n",
      "/bin/bash: scl: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `scl'\n",
      "/bin/bash: ml: line 1: syntax error: unexpected end of file\n",
      "/bin/bash: error importing function definition for `ml'\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:10<00:00,  3.66s/it]\n",
      "Epoch 1/1000\n",
      "Training Loss: 1.8082, Training Accuracy: 32.67%.\n",
      "Validation Loss: 1.6407, Validation Accuracy: 32.53%\n",
      "Improved! Saving model...\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:10<00:00,  3.65s/it]\n",
      "Epoch 2/1000\n",
      "Training Loss: 1.5611, Training Accuracy: 38.53%.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 284, in <module>\n",
      "    train(args)\n",
      "  File \"train.py\", line 200, in train\n",
      "    outputs = model(inputs)  # Forward pass\n",
      "  File \"/rds/general/user/ll1917/home/anaconda3/envs/esig-gp-esig-classifier-env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/rds/general/user/ll1917/home/esig/gp-esig-classifier/lib/model.py\", line 76, in forward\n",
      "    self.MeanSig = self.ExpectedSignatureLayer(path)\n",
      "  File \"/rds/general/user/ll1917/home/anaconda3/envs/esig-gp-esig-classifier-env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/rds/general/user/ll1917/home/esig/gp-esig-classifier/lib/utils.py\", line 197, in forward\n",
      "    norm = Normalization.apply(signatures, self.C, self.M, self.d, self.exponents, self.a)\n",
      "  File \"/rds/general/user/ll1917/home/esig/gp-esig-classifier/lib/utils.py\", line 56, in forward\n",
      "    norm = dilatation(input, C, a, M, d).to(input.device)\n",
      "  File \"/rds/general/user/ll1917/home/esig/gp-esig-classifier/lib/utils.py\", line 32, in dilatation\n",
      "    normQuad = 1 + np.sum(xNumpy[i, j]**2) # signature norm squared\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py -use_cuda -dataset Bidim -martingale_indices 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:esig-gp-esig-classifier-env]",
   "language": "python",
   "name": "conda-env-esig-gp-esig-classifier-env-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
