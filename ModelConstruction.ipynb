{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/frtrigg5/A-new-signature-model/blob/main/ModelConstruction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBM Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "abkdlzGBcdi5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ll1917\\Anaconda3\\envs\\esig-gp-esig-classifier-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# sys.path.append('/rds/general/user/ll1917/home/esig/gp-esig-classifier') # to add when running on remote Jupyter server\n",
    "# os.chdir('/rds/general/user/ll1917/home/esig/gp-esig-classifier') # to add when running on remote Jupyter server\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LqKs1fqyRSGR"
   },
   "outputs": [],
   "source": [
    "begin, end, number, division, dim = 0, 1, 100, 1, 1\n",
    "'''\n",
    "begin = first time steps\n",
    "end = last known time steps\n",
    "division = 1 means we are taking the middle points as new time instants -> L2 = L1 - 1\n",
    "number = L1\n",
    "dim = 1 - one dimensional\n",
    "'''\n",
    "\n",
    "# generating the time steps\n",
    "known_times = torch.linspace(begin, end, number)\n",
    "new_times = torch.zeros(division*(number-1))\n",
    "for i in range(0,(number-1)):\n",
    "  new_times[(division*i):(division*(i+1))] = torch.linspace(known_times[i], known_times[i+1], (division+2))[1:(1 + division)]\n",
    "\n",
    "# Length of known values and new values\n",
    "L1 = known_times.shape[0]\n",
    "L2 = new_times.shape[0]\n",
    "\n",
    "timesteps = torch.cat((known_times, new_times),axis=0)\n",
    "timesteps_sorted, order = torch.sort(timesteps)\n",
    "\n",
    "extended_order = torch.zeros(dim*order.size(0))\n",
    "for i in range(order.size(0)):\n",
    "  extended_order[(i*dim):((i+1)*dim)] = torch.arange(order[i]*dim, (order[i] + 1)*dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.synthetic import BM_sample, FBM_sample\n",
    "\n",
    "# construction of the dataset\n",
    "trShape, vlShape, testShape = 1000, 400, 600\n",
    "H = 0.26 # Hurst exponent\n",
    "\n",
    "known_times = np.linspace(begin, end, number) # istanti temporali noti\n",
    "div = 1 # quanti nuovi istanti temporali prendere tra due istanti noti\n",
    "new_times = np.zeros(div*(number - 1))\n",
    "for i in range((number-1)):\n",
    "  new_times[(div*i):(div*(i+1))] = np.linspace(known_times[i], known_times[i+1], (div+2))[1:(1+div)]\n",
    "\n",
    "L1 = known_times.size\n",
    "L2 = new_times.size\n",
    "\n",
    "timestamps = np.concatenate((known_times, new_times), axis=0)\n",
    "# time series train\n",
    "dataset_value = np.zeros(shape=[trShape, number])\n",
    "seed = 0\n",
    "for i in range(trShape//2):\n",
    "  dataset_value[i] = BM_sample(begin, end, number, seed=seed+i)[0]\n",
    "  dataset_value[i+trShape//2] = FBM_sample(begin, end, number, H, seed=seed+i)[0]\n",
    "\n",
    "# time series validation\n",
    "dataset_value2 = np.zeros(shape=[vlShape, number])\n",
    "seed = trShape//2\n",
    "for i in range(vlShape//2):\n",
    "  dataset_value2[i] = BM_sample(begin, end, number, seed=seed+i)[0]\n",
    "  dataset_value2[i+vlShape//2] = FBM_sample(begin, end, number, H, seed=seed+i)[0]\n",
    "\n",
    "# time series test\n",
    "dataset_value3 = np.zeros(shape=[testShape, number])\n",
    "seed = trShape//2 + vlShape//2\n",
    "for i in range(testShape//2):\n",
    "  dataset_value3[i] = BM_sample(begin, end, number, seed=seed+i)[0]\n",
    "  dataset_value3[i+testShape//2] = FBM_sample(begin, end, number, H, seed=seed+i)[0]\n",
    " \n",
    "# adding known and unknown time stamps\n",
    "time_data = np.zeros((trShape, L1 + L2))\n",
    "for i in range(trShape):\n",
    "  time_data[i] = timestamps\n",
    "\n",
    "time_data2 = np.zeros((vlShape, L1 + L2))\n",
    "for i in range(vlShape):\n",
    "  time_data2[i] = timestamps\n",
    "\n",
    "time_data3 = np.zeros((testShape, L1 + L2))\n",
    "for i in range(testShape):\n",
    "  time_data3[i] = timestamps  \n",
    "\n",
    "# full dataset train\n",
    "dataset = np.concatenate((time_data, dataset_value), axis=-1) # full dataset\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# full dataset validation\n",
    "dataset2 = np.concatenate((time_data2, dataset_value2), axis=-1) # full dataset\n",
    "dataset2 = dataset2.astype('float32')\n",
    "\n",
    "# full dataset test\n",
    "dataset3 = np.concatenate((time_data3, dataset_value3), axis=-1) # full dataset\n",
    "dataset3 = dataset3.astype('float32')\n",
    "\n",
    "# label construction\n",
    "y = np.zeros(trShape, dtype='uint8') # label 0 for Brownian Motion, 1 for FBM\n",
    "y[trShape//2:] = 1\n",
    "\n",
    "#label di validation\n",
    "y2 = np.zeros(vlShape, dtype='uint8')\n",
    "y2[vlShape//2:] = 1\n",
    "\n",
    "#label di test\n",
    "y3 = np.zeros(testShape, dtype='uint8') \n",
    "y3[testShape//2:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch = 60\n",
    "training_data = TensorDataset(torch.from_numpy(dataset), torch.from_numpy(y).long())\n",
    "train_loader = DataLoader(training_data, batch_size=batch, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(torch.from_numpy(dataset2), torch.from_numpy(y2).long())\n",
    "val_loader = DataLoader(val_data, batch_size=vlShape, shuffle=False)\n",
    "\n",
    "test_data = TensorDataset(torch.from_numpy(dataset3), torch.from_numpy(y3).long())\n",
    "test_loader = DataLoader(test_data, batch_size=testShape, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function to calculate accuracy\n",
    "def evaluate_accuracy(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradient calculation during evaluation\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)  # Total samples\n",
    "    \n",
    "    accuracy = correct / total * 100  # Calculate accuracy as a percentage\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model import GPES\n",
    "\n",
    "# # data params\n",
    "# dataset = 'MBvsFMB.npy'\n",
    "# train_X, train_y, test_X, test_y = np.load(os.path.join('.', 'data', dataset), allow_pickle=True)\n",
    "# train_samples, L1 = train_X.shape\n",
    "\n",
    "# model params\n",
    "L2 = L1 - 1\n",
    "alpha = L2\n",
    "level = 5\n",
    "number_classes = 2\n",
    "C = 8e2\n",
    "a = 1\n",
    "K = 30\n",
    "\n",
    "# fit params\n",
    "batch_size = 60 # int(0.25 * train_samples)\n",
    "lr = 8e-2\n",
    "epochs = 1500\n",
    "\n",
    "configs = {\n",
    "    'data': {\n",
    "        'dataset': dataset,\n",
    "        'L1': L1,\n",
    "    },\n",
    "    'model': {\n",
    "        'L2': L2,\n",
    "        'alpha': alpha, \n",
    "        'level': level, \n",
    "        'C': C, \n",
    "        'a': a, \n",
    "        'K': K\n",
    "    }, \n",
    "    'fit': {\n",
    "        'batch_size': batch_size,\n",
    "        'lr': lr,\n",
    "        'epochs': epochs\n",
    "    }\n",
    "}\n",
    "\n",
    "training_dir = os.path.join('checkpoints', 'run_004')\n",
    "os.makedirs(training_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = GPES(\n",
    "    L1=L1, \n",
    "    L2=L2, \n",
    "    dim=dim, \n",
    "    order=order, \n",
    "    extended_order=extended_order, \n",
    "    alpha=alpha, \n",
    "    level=level, \n",
    "    number_classes=number_classes, \n",
    "    C=C, \n",
    "    a=a, \n",
    "    K=K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    if not np.isfinite(model(inputs).detach().numpy()).all():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "training_dir = os.path.join('checkpoints', 'run_001')\n",
    "os.makedirs(training_dir, exist_ok=True)\n",
    "\n",
    "# Initialize loss function, and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 300\n",
    "patience = 10\n",
    "\n",
    "# Define the path to save the checkpoint\n",
    "checkpoint_path = os.path.join(training_dir, 'checkpoint.pth')\n",
    "\n",
    "# Initialize best_loss and early_stopping_counter\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Load from checkpoint if it exists\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    early_stopping_counter = checkpoint['early_stopping_counter']\n",
    "else:\n",
    "    start_epoch = 0  # Start from the beginning if no checkpoint exists\n",
    "\n",
    "exit = False\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        if not np.isfinite(model(inputs).detach().numpy()).all():\n",
    "            exit = True\n",
    "            break\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize weights\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    if exit:\n",
    "        break\n",
    "    \n",
    "    # Average loss for the training epoch\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():  # No gradient calculation for validation\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute validation loss\n",
    "            val_running_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "    # Average loss for the validation epoch\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Calculate and print accuracy on validation set\n",
    "    accuracy = evaluate_accuracy(model, val_loader)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        early_stopping_counter = 0  # Reset counter if loss improves\n",
    "        print(\"Improved! Saving model...\")\n",
    "        torch.save(model.state_dict(), os.path.join(training_dir, 'best_model.pth'))  # Save the best model\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break  # Stop training if patience is exceeded\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,  # Save next epoch\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'early_stopping_counter': early_stopping_counter,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.40%\n"
     ]
    }
   ],
   "source": [
    "training_dir = os.path.join('checkpoints', 'run_001')\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(os.path.join(training_dir, 'best_model.pth')))\n",
    "\n",
    "# Calculate and print accuracy on validation set\n",
    "accuracy = evaluate_accuracy(model, train_loader)\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "esig-gp-esig-classifier-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
